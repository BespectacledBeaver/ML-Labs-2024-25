{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2CSSID Lab02. Naïve Bayes\n",
    "\n",
    "<p style='text-align: right;font-style: italic;'>Designed by: Mr. Abdelkrime Aries</p>\n",
    "\n",
    "In this lab, we will learn about Naive Bayes by testing 2 implementations:\n",
    "- Multinomial Naïve Bayes\n",
    "- Gaussian Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Team:**\n",
    "- **Member 01**: ...\n",
    "- **Member 02**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:03:56) [MSC v.1929 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, timeit\n",
    "from typing          import Tuple, List, Type\n",
    "from collections.abc import Callable\n",
    "\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.26.4', '2.2.2', '3.8.4')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy             as np\n",
    "import pandas            as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "np.__version__, pd.__version__, matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.2'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "from sklearn.naive_bayes   import CategoricalNB\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics       import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection         import train_test_split\n",
    "from sklearn.naive_bayes             import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.tree                    import DecisionTreeClassifier\n",
    "from sklearn.metrics                 import precision_score, recall_score\n",
    "import timeit\n",
    "\n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Algorithms implementation\n",
    "\n",
    "In this section, we will try to implement multinomial Naive Bayes.\n",
    "\n",
    "\n",
    "**>> Try to use \"numpy\" which will save a lot of time and effort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset play \n",
    "\n",
    "# outlook & temperature & humidity & windy\n",
    "Xplay = np.array([\n",
    "    ['sunny'   , 'hot' , 'high'  , 'no'],\n",
    "    ['sunny'   , 'hot' , 'high'  , 'yes'],\n",
    "    ['overcast', 'hot' , 'high'  , 'no'],\n",
    "    ['rainy'   , 'mild', 'high'  , 'no'],\n",
    "    ['rainy'   , 'cool', 'normal', 'no'],\n",
    "    ['rainy'   , 'cool', 'normal', 'yes'],\n",
    "    ['overcast', 'cool', 'normal', 'yes'],\n",
    "    ['sunny'   , 'mild', 'high'  , 'no'],\n",
    "    ['sunny'   , 'cool', 'normal', 'no'],\n",
    "    ['rainy'   , 'mild', 'normal', 'no'],\n",
    "    ['sunny'   , 'mild', 'normal', 'yes'],\n",
    "    ['overcast', 'mild', 'high'  , 'yes'],\n",
    "    ['overcast', 'hot' , 'normal', 'no'],\n",
    "    ['rainy'   , 'mild', 'high'  , 'yes']\n",
    "])\n",
    "\n",
    "Yplay = np.array([\n",
    "    'no', \n",
    "    'no', \n",
    "    'yes', \n",
    "    'yes', \n",
    "    'yes', \n",
    "    'no', \n",
    "    'yes', \n",
    "    'no', \n",
    "    'yes', \n",
    "    'yes', \n",
    "    'yes', \n",
    "    'yes', \n",
    "    'yes', \n",
    "    'no'\n",
    "])\n",
    "\n",
    "len(Xplay), len(Yplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# height & weight & footsize & person\n",
    "Xperson = np.array([\n",
    "    [182., 81.6, 30.],\n",
    "    [180., 86.2, 28.],\n",
    "    [170., 77.1, 30.],\n",
    "    [180., 74.8, 25.],\n",
    "    [152., 45.4, 15.],\n",
    "    [168., 68.0, 20.],\n",
    "    [165., 59.0, 18.],\n",
    "    [175., 68.0, 23.]\n",
    "])\n",
    "\n",
    "Yperson = np.array([\n",
    "    'male', 'male', 'male', 'male',\n",
    "    'female', 'female', 'female', 'female'\n",
    "])\n",
    "\n",
    "len(Xperson), len(Yperson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1. Prior statistics\n",
    "\n",
    "Given an output list $Y[M]$, the probability of each class $c$ is estimated as:\n",
    "$$p(c) = \\frac{\\#(Y = c)}{|Y|}$$\n",
    "\n",
    "In here, we want to store the frequencies of different classes.\n",
    "Our function must return two lists:\n",
    "- One containing the names of unique classes.\n",
    "- Another containing their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array(['no', 'yes'], dtype='<U3'), array([5, 9], dtype=int64)),\n",
       " (array(['female', 'male'], dtype='<U6'), array([4, 4], dtype=int64)))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Prior statistics\n",
    "def fit_prior(Y: 'np.ndarray[M](str)') -> ['np.ndarray[K](str)', 'np.ndarray[K](int)']: \n",
    "    return np.unique(Y,return_counts=True)\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# ((array(['no', 'yes'], dtype='<U3'), array([5, 9])),\n",
    "#  (array(['female', 'male'], dtype='<U6'), array([4, 4])))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "fit_prior(Yplay), fit_prior(Yperson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Multinomial Law\n",
    "\n",
    "In this section, we will implement multinomial naive Bayes from scratch using Numpy.\n",
    "\n",
    "#### I.2.1. Multinomial Likelihood statistics\n",
    "\n",
    "Given:\n",
    "- $A$: a categorical feature\n",
    "- $Y$: the ouput\n",
    "- $C$: the classes\n",
    "\n",
    "The function takes as argument $A, Y, C$ previously described.\n",
    "It must return:\n",
    "- $V$: unique values of this feature (feature's categories)\n",
    "- $S[|C|, |V|]$: a matrix containing count $\\#(Y = c \\wedge A = v),\\, \\forall c \\in C, \\forall v \\in A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array(['overcast', 'rainy', 'sunny'], dtype='<U8'),\n",
       "  array([[0., 2., 3.],\n",
       "         [4., 3., 2.]])),\n",
       " (array(['cool', 'hot', 'mild'], dtype='<U8'),\n",
       "  array([[1., 2., 2.],\n",
       "         [3., 2., 4.]])))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Multinomial Likelihood statistics\n",
    "def fit_multinomial_likelihood(A: 'np.ndarray[M](str)', \n",
    "                               Y: 'np.ndarray[M](str)', \n",
    "                               C: 'np.ndarray[C](str)'\n",
    "                               ) -> ['np.ndarray[V](str)', 'np.ndarray[C, V](int)']:\n",
    "    V = np.unique(A)\n",
    "    S = np.zeros((len(C),len(V)))\n",
    "    for i in range(len(C)):\n",
    "        for j in range(len(V)):\n",
    "            for k in range(len(A)):\n",
    "                if A[k]==V[j] and Y[k]==C[i]: S[i,j]+=1\n",
    "    return (V,S)\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# ((array(['overcast', 'rainy', 'sunny'], dtype='<U8'),\n",
    "#   array([[0, 2, 3],\n",
    "#          [4, 3, 2]])),\n",
    "#  (array(['cool', 'hot', 'mild'], dtype='<U8'),\n",
    "#   array([[1, 2, 2],\n",
    "#          [3, 2, 4]])))\n",
    "#---------------------------------------------------------------------\n",
    "C_t = np.array(['no', 'yes'])\n",
    "fit_multinomial_likelihood(Xplay[:, 0], Yplay, C_t), fit_multinomial_likelihood(Xplay[:, 1], Yplay, C_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.2. Multinomial Likelihood training\n",
    "\n",
    "**Nothing to code here, although you have to know how it functions for next use**\n",
    "\n",
    "This function aims to generate parameters $\\theta$. \n",
    "In our case, paramters are diffrent from those of *logistic regrssion*.\n",
    "They are a dictionary (map) with two entries:\n",
    "- \"prior\": a dictionary having \"vocab\" a list of values and \"freq\" a list of their respective frequencies.\n",
    "- \"likelihood\": a list of dictionaries representing statistics of each feature (the same order of $X$ features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prior': {'vocab': array(['no', 'yes'], dtype='<U3'),\n",
       "  'freq': array([5, 9], dtype=int64)},\n",
       " 'likelihood': [{'vocab': array(['overcast', 'rainy', 'sunny'], dtype='<U8'),\n",
       "   'freq': array([[0., 2., 3.],\n",
       "          [4., 3., 2.]])},\n",
       "  {'vocab': array(['cool', 'hot', 'mild'], dtype='<U8'),\n",
       "   'freq': array([[1., 2., 2.],\n",
       "          [3., 2., 4.]])},\n",
       "  {'vocab': array(['high', 'normal'], dtype='<U8'),\n",
       "   'freq': array([[4., 1.],\n",
       "          [3., 6.]])},\n",
       "  {'vocab': array(['no', 'yes'], dtype='<U8'),\n",
       "   'freq': array([[2., 3.],\n",
       "          [6., 3.]])}]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit_multinomial_NB(X: 'np.ndarray[M, N](str)', \n",
    "                       Y: 'np.ndarray[M](str)'\n",
    "                       ) -> object: \n",
    "    \n",
    "    Theta   = {'prior': {}, 'likelihood': []}\n",
    "\n",
    "    Theta['prior']['vocab'], Theta['prior']['freq'] = fit_prior(Y)\n",
    "\n",
    "    for j in range(X.shape[1]): \n",
    "        likelihood = {}\n",
    "        likelihood['vocab'], likelihood['freq'] = fit_multinomial_likelihood(X[:, j], Y, Theta['prior']['vocab'])\n",
    "        Theta['likelihood'].append(likelihood)\n",
    "    \n",
    "    return Theta\n",
    "\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# {'prior': {'vocab': array(['no', 'yes'], dtype='<U3'), 'freq': array([5, 9])},\n",
    "#  'likelihood': [{'vocab': array(['overcast', 'rainy', 'sunny'], dtype='<U8'),\n",
    "#    'freq': array([[0, 2, 3],\n",
    "#           [4, 3, 2]])},\n",
    "#   {'vocab': array(['cool', 'hot', 'mild'], dtype='<U8'),\n",
    "#    'freq': array([[1, 2, 2],\n",
    "#           [3, 2, 4]])},\n",
    "#   {'vocab': array(['high', 'normal'], dtype='<U8'),\n",
    "#    'freq': array([[4, 1],\n",
    "#           [3, 6]])},\n",
    "#   {'vocab': array(['no', 'yes'], dtype='<U8'),\n",
    "#    'freq': array([[2, 3],\n",
    "#           [6, 3]])}]}\n",
    "#---------------------------------------------------------------------\n",
    "Theta_play = fit_multinomial_NB(Xplay, Yplay)\n",
    "\n",
    "Theta_play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.3. Multinomial Likelihood prediction\n",
    "\n",
    "Given:\n",
    "- $A$: a categorical feature\n",
    "- $V$: unique values of this feature (feature's categories)\n",
    "- $Y$: the ouput\n",
    "- $C$: the classes\n",
    "- $\\alpha$: smoothing factor\n",
    "\n",
    "Log likelihood is calculated as:\n",
    "$$ \\log p(A=v|Y=c) = \\log(\\#(Y = k \\wedge A = v) + \\alpha) - \\log(\\#(y = k) + \\alpha * |V|)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, -1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can use this function in the next implimentation\n",
    "# It takes a list of unique values V and a given value v\n",
    "# It returns the position of v in V\n",
    "# If v does not exist in V, it returns -1\n",
    "def find_idx(V: np.ndarray, v: str) -> int:\n",
    "    k = np.argwhere(V == v).flatten()\n",
    "    if len(k):\n",
    "        return k[0]\n",
    "    return -1\n",
    "\n",
    "V_t = np.array(['One', 'Two', 'Three'])\n",
    "find_idx(V_t, 'Two'), find_idx(V_t, 'Four')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((-0.916290731874155, -1.0986122886681098),\n",
       " (-2.0794415416798357, -2.4849066497880004))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Multinomial Likelihood prediction\n",
    "def predict_multinomial_NB1(v: str, \n",
    "                            j: int,\n",
    "                            Theta: object,  \n",
    "                            alpha: float = 0.\n",
    "                            ) -> 'np.ndarray[C](float)':\n",
    "    \n",
    "    for i in range(len(Theta['likelihood'])) :\n",
    "        idx = find_idx(Theta['likelihood'][i]['vocab'],v)\n",
    "        if idx != -1:\n",
    "            return (np.log(Theta['likelihood'][i]['freq'][0][idx]+alpha)\n",
    "                    -np.log(np.sum(Theta['likelihood'][i]['freq'][0],axis=0)+alpha*len(Theta['likelihood'][i]['vocab'])),\n",
    "                    np.log(Theta['likelihood'][i]['freq'][1][idx]+alpha)\n",
    "                    -np.log(np.sum(Theta['likelihood'][i]['freq'][1],axis=0)+alpha*len(Theta['likelihood'][i]['vocab'])))\n",
    "    \n",
    "    return (np.log(alpha)\n",
    "            -np.log(np.sum(Theta['likelihood'][0]['freq'][0],axis=0)+alpha*len(Theta['likelihood'][0]['vocab'])),\n",
    "            np.log(alpha)\n",
    "            -np.log(np.sum(Theta['likelihood'][0]['freq'][1],axis=0)+alpha*len(Theta['likelihood'][0]['vocab'])))\n",
    "                    \n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# (array([-0.91629073, -1.09861229]), array([-2.07944154, -2.48490665]))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "X_t = np.array([\n",
    "    ['rainy', 'cool', 'normal', 'yes'],\n",
    "    ['snowy', 'cool', 'normal', 'yes'],\n",
    "    ['sunny', 'hot' , 'normal', 'no']\n",
    "])\n",
    "\n",
    "predict_multinomial_NB1('rainy', 0, Theta_play, alpha=0.), \\\n",
    "    predict_multinomial_NB1('snowy', 0, Theta_play, alpha=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3. Normal (Gaussian) Law\n",
    "\n",
    "In this section, we will implement gaussian naive Bayes from scratch using Numpy.\n",
    "\n",
    "#### I.3.1. Gaussian Likelihood statistics\n",
    "\n",
    "Given:\n",
    "- $A$: a categorical feature\n",
    "- $Y$: the ouput\n",
    "- $C$: the classes\n",
    "\n",
    "The function takes as argument $A, Y, C$ previously described.\n",
    "It must return $S[|C|, 2, N]$; a tensor having these dimensions:\n",
    "- first dimension: each element represents one class's statistics\n",
    "- second dimension: 1st element represents means; 2ns element represents variances\n",
    "- third dimension: each element represents mean/variance of the respective feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[165.        ,  60.1       ,  19.        ],\n",
       "        [ 92.66666667, 114.04      ,  11.33333333]],\n",
       "\n",
       "       [[178.        ,  79.925     ,  28.25      ],\n",
       "        [ 29.33333333,  25.47583333,   5.58333333]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Gaussian Likelihood statistics\n",
    "def fit_gaussian_likelihood(X: 'np.ndarray[M](float)', \n",
    "                            Y: 'np.ndarray[M](str)', \n",
    "                            C: 'np.ndarray[C](str)'\n",
    "                            ) -> 'np.ndarray[C, 2, N](float)': \n",
    "    S = np.zeros((len(C), 2, X.shape[1]))\n",
    "    for i in range(len(C)):\n",
    "        X_class = X[Y == C[i]]\n",
    "\n",
    "        means = np.mean(X_class, axis=0)\n",
    "        variances = np.var(X_class, axis=0,ddof=1)\n",
    "\n",
    "        S[i, 0, :] = means\n",
    "        S[i, 1, :] = variances\n",
    "    \n",
    "    return S\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# array([[[165.        ,  60.1       ,  19.        ],\n",
    "#         [ 92.66666667, 114.04      ,  11.33333333]],\n",
    "\n",
    "#        [[178.        ,  79.925     ,  28.25      ],\n",
    "#         [ 29.33333333,  25.47583333,   5.58333333]]])\n",
    "#---------------------------------------------------------------------\n",
    "C_t = np.array(['female', 'male'])\n",
    "fit_gaussian_likelihood(Xperson, Yperson, C_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.3.2. Gaussian Likelihood training\n",
    "\n",
    "**Nothing to code here, although you have to know how it functions for next use**\n",
    "\n",
    "This function aims to generate parameters $\\theta$. \n",
    "In our case, paramters are diffrent from those of *logistic regrssion*.\n",
    "They are a dictionary (map) with two entries:\n",
    "- \"prior\": a dictionary having \"vocab\" a list of values and \"freq\" a list of their respective frequencies.\n",
    "- \"likelihood\": a tensor of shape $[|C|, 2, N]$ containing likelihood statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prior': {'vocab': array(['female', 'male'], dtype='<U6'),\n",
       "  'freq': array([4, 4], dtype=int64)},\n",
       " 'likelihood': array([[[165.        ,  60.1       ,  19.        ],\n",
       "         [ 92.66666667, 114.04      ,  11.33333333]],\n",
       " \n",
       "        [[178.        ,  79.925     ,  28.25      ],\n",
       "         [ 29.33333333,  25.47583333,   5.58333333]]])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit_gaussian_NB(X: 'np.ndarray[M, N](str)', \n",
    "                    Y: 'np.ndarray[M](str)'\n",
    "                    ) -> object: \n",
    "    \n",
    "    Theta   = {'prior': {}, 'likelihood': []}\n",
    "\n",
    "    Theta['prior']['vocab'], Theta['prior']['freq'] = fit_prior(Y)\n",
    "    Theta['likelihood'] = fit_gaussian_likelihood(X, Y, Theta['prior']['vocab'])\n",
    "\n",
    "    return Theta\n",
    "\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# {'prior': {'vocab': array(['female', 'male'], dtype='<U6'),\n",
    "#   'freq': array([4, 4])},\n",
    "#  'likelihood': array([[[165.        ,  60.1       ,  19.        ],\n",
    "#          [ 92.66666667, 114.04      ,  11.33333333]],\n",
    " \n",
    "#         [[178.        ,  79.925     ,  28.25      ],\n",
    "#          [ 29.33333333,  25.47583333,   5.58333333]]])}\n",
    "#---------------------------------------------------------------------\n",
    "Theta_person = fit_gaussian_NB(Xperson, Yperson)\n",
    "\n",
    "Theta_person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.4. Gaussian Likelihood prediction\n",
    "\n",
    "Given:\n",
    "- $A$: a numerical feature\n",
    "- $\\mu_{Ac}$: mean of values of feature $A$ having $c$ as class\n",
    "- $\\sigma_{Ac}$: variance of values of feature $A$ having $c$ as class\n",
    "- $Y$: the output\n",
    "- $C$: the classes\n",
    "\n",
    "Log likelihood is calculated as:\n",
    "$$ \\log p(A=v|Y=c) = \\frac{-(v-\\mu_{Ac})^2}{2 \\sigma_{Ac}^2} - \\log(\\sqrt{2\\pi \\sigma_{Ac}^2})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-4.93164438, -3.03443716]), array([0.00721463, 0.04810173]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Gaussian Likelihood prediction\n",
    "def predict_gaussian_NB1(v: str, \n",
    "                         j: int,\n",
    "                         Theta: object,  \n",
    "                         alpha: float = 0. # this is just added for compatibility\n",
    "                         ) -> 'np.ndarray[C](float)':\n",
    "    \n",
    "    return (-((v - Theta['likelihood'][:, 0, j]) ** 2) \n",
    "            / (2 * Theta['likelihood'][:, 1, j]) \n",
    "            - np.log(np.sqrt(2 * np.pi * Theta['likelihood'][:, 1, j])))\n",
    "\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# (array([-4.93164438, -3.03443716]), array([0.00721463, 0.04810173]))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "pp = predict_gaussian_NB1(183, 0, Theta_person)\n",
    "\n",
    "pp, np.exp(pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.4. Final prediction\n",
    "\n",
    "Our goal is to calculate approximate log probabilities of all classes given a sample:\n",
    "$$\\log P(y=c_k | \\overrightarrow{x} = \\overrightarrow{f})  \\approx \\log P(y=c_k) + \\sum\\limits_{f_j \\in \\overrightarrow{f}} \\log P(f_j = x_j|y=c_k)$$\n",
    "\n",
    "This function takes:\n",
    "- $X^{(i)}$ one sample with $N$ features\n",
    "- $\\theta$ parameters (either those of multinomial or gaussian)\n",
    "- $pred_{fct}$ a function to predict one feauture (either multinomial or gaussian)\n",
    "- add_prior: if True, add prior probability\n",
    "- $\\alpha$ smoothing factor (passing it to gaussian function will do nothing)\n",
    "\n",
    "It must return a vector of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-3.59617006, -4.95406494]),\n",
       " array([-2.56655064, -4.51223219]),\n",
       " array([-2.85774653, -4.23617476]),\n",
       " array([-10.401093  , -22.03977023]))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Final prediction\n",
    "def predict_NB1(Xi    : 'np.ndarray[N]', \n",
    "                Theta: object,  \n",
    "                pred_fct: Callable,\n",
    "                add_prior: bool  = True,\n",
    "                alpha: float = 1.0\n",
    "                ) -> 'np.ndarray[C](float)':\n",
    "    S = [0,0]\n",
    "    for i in range(len(Xi)):\n",
    "        S = np.add(S,pred_fct(Xi[i],i,Theta,alpha))\n",
    "    if add_prior:\n",
    "        S = np.add(S,np.log(Theta['prior']['freq']/np.sum(Theta['prior']['freq'])))\n",
    "    return S\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# (array([-3.59617006, -4.95406494]), \n",
    "#  array([-2.56655064, -4.51223219]), \n",
    "#  array([-2.85774653, -4.23617476]), \n",
    "#  array([-10.401093 , -22.03977023]))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "X_t1 = np.array(['sunny', 'hot' , 'high', 'no'])\n",
    "X_t2 = np.array([183., 59., 20.])\n",
    "\n",
    "predict_NB1(X_t1, Theta_play, predict_multinomial_NB1, add_prior=True, alpha=0.0), \\\n",
    "predict_NB1(X_t1, Theta_play, predict_multinomial_NB1, add_prior=False, alpha=0.0), \\\n",
    "predict_NB1(X_t1, Theta_play, predict_multinomial_NB1, add_prior=False, alpha=1.0), \\\n",
    "predict_NB1(X_t2, Theta_person, predict_gaussian_NB1, add_prior=False),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.5. Final product\n",
    "\n",
    "**>> Nothing to code here**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['yes', 'yes', 'no'], dtype='<U3'),\n",
       " array([[-5.20912179, -4.10264337],\n",
       "        [-6.30773408, -5.48893773],\n",
       "        [-3.88736595, -4.67800751]]),\n",
       " array(['female', 'male'], dtype='<U6'),\n",
       " array([[-11.09424018, -22.73291741],\n",
       "        [-15.27968966, -12.41764665]]))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NaiveBayes(object): \n",
    "\n",
    "    def __init__(self, multinomial=True):\n",
    "        if multinomial:\n",
    "            self.train = fit_multinomial_NB\n",
    "            self.pred = predict_multinomial_NB1\n",
    "        else:\n",
    "            self.train = fit_gaussian_NB\n",
    "            self.pred = predict_gaussian_NB1\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        self.Theta = self.train(X, Y)\n",
    "    \n",
    "    def predict(self, X, add_prior=True, prob=False, alpha=0.): \n",
    "        Y_pred = []\n",
    "        for i in range(len(X)): \n",
    "            Y_pred.append(predict_NB1(\n",
    "                X[i,:], self.Theta, self.pred, add_prior=add_prior, alpha=alpha\n",
    "                ))\n",
    "        \n",
    "        Y_pred = np.array(Y_pred)\n",
    "\n",
    "        if prob:\n",
    "            return Y_pred\n",
    "\n",
    "        return np.choose(np.argmax(Y_pred, axis=1), self.Theta['prior']['vocab'])\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result:\n",
    "# (array(['yes', 'yes', 'no'], dtype='<U3'),\n",
    "# array([[-5.20912179, -4.10264337],\n",
    "#       [-6.30773408, -5.48893773],\n",
    "#       [-3.88736595, -4.67800751]]),\n",
    "# array(['female', 'male'], dtype='<U6'),\n",
    "# array([[-11.09424018, -22.73291741],\n",
    "#       [-15.27968966, -12.41764665]]))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "multinomial_nb = NaiveBayes()\n",
    "multinomial_nb.fit(Xplay, Yplay)\n",
    "\n",
    "gaussian_nb = NaiveBayes(multinomial=False)\n",
    "gaussian_nb.fit(Xperson, Yperson)\n",
    "\n",
    "X_t1 = np.array([\n",
    "    ['rainy', 'cool', 'normal', 'yes'],\n",
    "    ['snowy', 'cool', 'normal', 'yes'],\n",
    "    ['sunny', 'hot' , 'high', 'no']\n",
    "])\n",
    "\n",
    "X_t2 = np.array([\n",
    "    [183., 59., 20.],\n",
    "    [175., 65., 30.]\n",
    "])\n",
    "\n",
    "\n",
    "multinomial_nb.predict(X_t1, alpha=1.), \\\n",
    "    multinomial_nb.predict(X_t1, alpha=1., prob=True), \\\n",
    "    gaussian_nb.predict(X_t2), \\\n",
    "    gaussian_nb.predict(X_t2, prob=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Application and Analysis\n",
    "\n",
    "In this section, we will test different concepts by running an experiment, formulating a hypothesis and trying to justify it.\n",
    "\n",
    "### II.1. Prior probability \n",
    "\n",
    "We want to test the effect of prior probability.\n",
    "To do this, we trained two models:\n",
    "1. With prior probability\n",
    "1. Without prior probability (It considers a uniform distribution of classes)\n",
    "\n",
    "To test whether the models have adapted well to the training dataset, we will test them on the same dataset and calculate the classification ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considring prior probability\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       1.00      0.80      0.89         5\n",
      "         yes       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.95      0.90      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n",
      "No prior probability\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.67      0.80      0.73         5\n",
      "         yes       0.88      0.78      0.82         9\n",
      "\n",
      "    accuracy                           0.79        14\n",
      "   macro avg       0.77      0.79      0.78        14\n",
      "weighted avg       0.80      0.79      0.79        14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_withPrior     = CategoricalNB(alpha=1.0, fit_prior=True )\n",
    "nb_noPrior       = CategoricalNB(alpha=1.0, fit_prior=False)\n",
    "\n",
    "enc         = OrdinalEncoder()\n",
    "Xplay_tf    = enc.fit_transform(Xplay)\n",
    "nb_withPrior.fit(Xplay_tf, Yplay)\n",
    "nb_noPrior.fit(Xplay_tf, Yplay)\n",
    "\n",
    "Ypred_withPrior = nb_withPrior.predict(Xplay_tf)\n",
    "Ypred_noPrior = nb_noPrior.predict(Xplay_tf)\n",
    "\n",
    "\n",
    "print( 'Considring prior probability'  )\n",
    "print(classification_report(Yplay, Ypred_withPrior))\n",
    "\n",
    "print( 'No prior probability'  )\n",
    "print(classification_report(Yplay, Ypred_noPrior))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyze the results**\n",
    "\n",
    "1. What do you notice, indicating if prior probability is useful in this case?\n",
    "1. How does this probability affect the outcome?\n",
    "1. When are we sure that using this probability is unnecessary? \n",
    "\n",
    "**Answer**\n",
    "\n",
    "1. We notice that the model w prior probability is more accurate (93%) than the model w/o prior probability. The performance measures are better w prior probability, making it useful in this case because the model identified all 'Yes' true positives (yes recall 1.00) and none of the 'No' false positives (no percision 1.00).\n",
    "1. This probability balances the weighting of the dataset (#no = 5 < 9 = #yes).\n",
    "1. This probability is unnecessary when the dataset is balanced (uniformly distributed among the classes) or if we want to give the same prior probability to classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2. Smoothing\n",
    "\n",
    "We want to test the Lidstone smoothing's effect.\n",
    "To do this, we trained three models:\n",
    "1. alpha = 1 (Laplace smoothing)\n",
    "1. alpha = 0.5\n",
    "1. alpha = 0 (without smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       1.00      0.80      0.89         5\n",
      "         yes       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.95      0.90      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n",
      "Alpha = 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       1.00      0.80      0.89         5\n",
      "         yes       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.95      0.90      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n",
      "Alpha = 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       1.00      0.80      0.89         5\n",
      "         yes       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.95      0.90      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\inruinnomore\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:1504: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(smoothed_cat_count) - np.log(smoothed_class_count.reshape(-1, 1))\n"
     ]
    }
   ],
   "source": [
    "NBC_10 = CategoricalNB(alpha = 1.0 )\n",
    "NBC_05 = CategoricalNB(alpha = 0.5 )\n",
    "NBC_00 = CategoricalNB(alpha = 0.0 )\n",
    "\n",
    "NBC_10.fit( Xplay_tf,   Yplay )\n",
    "NBC_05.fit( Xplay_tf,   Yplay )\n",
    "NBC_00.fit( Xplay_tf,   Yplay )\n",
    "\n",
    "Y_10   = NBC_10.predict(Xplay_tf)\n",
    "Y_05   = NBC_05.predict(Xplay_tf)\n",
    "Y_00   = NBC_00.predict(Xplay_tf)\n",
    "\n",
    "\n",
    "print(                'Alpha = 1.0'                        )\n",
    "print(classification_report(Yplay, Y_10, zero_division=0.0))\n",
    "\n",
    "print(                'Alpha = 0.5'                        )\n",
    "print(classification_report(Yplay, Y_05, zero_division=0.0))\n",
    "\n",
    "print(                'Alpha = 0.0'                        )\n",
    "print(classification_report(Yplay, Y_00, zero_division=0.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyze the results**\n",
    "\n",
    "1. What do you notice, indicating if smoothing affects performance in this case?\n",
    "1. Based on the past answer, Why? \n",
    "1. Why do we get a \"RuntimeWarning: divide by zero\" error? \n",
    "1. What is the benefit of smoothing (generally; not just for this case)?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "1. Performance measures are the same across all three models with varying smoothing factors, thus smoothing doesn't affect performance in this case.\n",
    "1. Based on the *Algorithms Implementation* segment, smoothing is meant for datasets that include features with zero-probability. As that isn't our case, it doesn't affect the models.\n",
    "1. Due to the third model having a smoothing factor of alpha=0, one of the functions in the sklearn implementation of NB tries to compute the natural logarithme of alpha=0 (mathematically undefined).\n",
    "1. Avoids having zero-probability cases, particularly interesting when we don't have much data points or when there are unseen values in the dataset. ***APPARENTLY THE TEACHER SAID SMOOTHING IS ALSO BENEFITIAL WHEN ADDING NEW DATA.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3. Naive Bayes performance\n",
    "\n",
    "Naive Bayes is known to generate powerful models when it comes to classifying textual documents.\n",
    "We want to test this proposition using spam detection over [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset).\n",
    "\n",
    "Each message is represented using term frequency (TF), where a word is considered as a feature.\n",
    "In this case, a message is represented by a vector of frequencies (how many times each word appeared in the message).\n",
    "We want to compare these models:\n",
    "1. Multinomial Naive Bayes (MNB)\n",
    "1. Gaussian Naive Bayes (GNB)\n",
    "1. Logistic Regression (LR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class\n",
       "0  Go until jurong point, crazy.. Available only ...   ham\n",
       "1                      Ok lar... Joking wif u oni...   ham\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...  spam\n",
       "3  U dun say so early hor... U c already then say...   ham\n",
       "4  Nah I don't think he goes to usf, he lives aro...   ham"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the dataset\n",
    "messages = pd.read_csv('data/spam.csv', encoding='latin-1')\n",
    "# renaming features: text and class\n",
    "messages = messages.rename(columns={'v1': 'class', 'v2': 'text'})\n",
    "# keeping only these two features\n",
    "messages = messages.filter(['text', 'class'])\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Train time</th>\n",
       "      <th>Test time</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multinomial Naive Bayes (MNB)</td>\n",
       "      <td>1.667836</td>\n",
       "      <td>0.070039</td>\n",
       "      <td>0.987179</td>\n",
       "      <td>0.927711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gaussian Naive Bayes  (GNB)</td>\n",
       "      <td>0.958312</td>\n",
       "      <td>0.286772</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.891566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression (LR)</td>\n",
       "      <td>0.962181</td>\n",
       "      <td>0.046691</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.855422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Algorithm  Train time  Test time  Precision    Recall\n",
       "0  Multinomial Naive Bayes (MNB)    1.667836   0.070039   0.987179  0.927711\n",
       "1    Gaussian Naive Bayes  (GNB)    0.958312   0.286772   0.616667  0.891566\n",
       "2       Logistic Regression (LR)    0.962181   0.046691   0.986111  0.855422"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\n",
    "    MultinomialNB(),\n",
    "    GaussianNB(),\n",
    "    LogisticRegression(solver='lbfgs'),\n",
    "    #solver=sag is slower; so I chose the fastest\n",
    "]\n",
    "\n",
    "algos = [\n",
    "    'Multinomial Naive Bayes (MNB)', \n",
    "    'Gaussian Naive Bayes  (GNB)', \n",
    "    'Logistic Regression (LR)', \n",
    "]\n",
    "\n",
    "perf = {\n",
    "    'train_time': [],\n",
    "    'test_time' : [],\n",
    "    'recall'    : [],\n",
    "    'precision' : []\n",
    "}\n",
    "\n",
    "\n",
    "msg_train, msg_test, Y_train, Y_test = train_test_split(messages['text'] ,\n",
    "                                                        messages['class'],\n",
    "                                                        test_size    = 0.2, \n",
    "                                                        random_state = 0  )\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train          = count_vectorizer.fit_transform(msg_train).toarray()\n",
    "X_test           = count_vectorizer.transform    (msg_test ).toarray()\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    # ==================================\n",
    "    # TRAIN \n",
    "    # ==================================\n",
    "    start_time = timeit.default_timer()\n",
    "    model.fit(X_train, Y_train)\n",
    "    perf['train_time'].append(timeit.default_timer() - start_time)\n",
    "    \n",
    "    # ==================================\n",
    "    # TEST \n",
    "    # ==================================\n",
    "    start_time = timeit.default_timer()\n",
    "    Y_pred     = model.predict(X_test)\n",
    "    perf['test_time'].append(timeit.default_timer() - start_time)\n",
    "    \n",
    "    # ==================================\n",
    "    # PERFORMANCE \n",
    "    # ==================================\n",
    "    # In here, we are interrested in \"spam\" class which is our positive class\n",
    "    perf['precision'].append(precision_score(Y_test, Y_pred, pos_label='spam'))\n",
    "    perf['recall'   ].append(recall_score   (Y_test, Y_pred, pos_label='spam'))\n",
    "\n",
    "    \n",
    "pd.DataFrame({\n",
    "    'Algorithm' : algos,\n",
    "    'Train time': perf['train_time'],\n",
    "    'Test time' : perf['test_time'],\n",
    "    'Precision' : perf['precision'],\n",
    "    'Recall'    : perf['recall']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyze the results**\n",
    "\n",
    "1. What do you notice about training time? (order the algorithms)\n",
    "1. Why did we get these results based on the algorithms? (discuss each algorithm with respect to training time)\n",
    "1. What do you notice about the testing time? (order the algorithms)\n",
    "1. Why did we get these results based on the algorithms? (discuss each algorithm with respect to testing time)\n",
    "1. Why is the Gaussian model less efficient than the multinomial based on the nature of the two algorithms?\n",
    "1. Why is the Gaussian model less efficient than the multinomial based on the nature of the problem/data?\n",
    "1. How Multinomial NB's implementation affect the training/test time? (store statistics vs. store probabilities)\n",
    "1. Which one is more adequate for updating the model with new data? explain.\n",
    "\n",
    "**Answer**\n",
    "\n",
    "1. We notice that: (Fastest) MNB > GNB > LR (Slowest), with LR having nearly double the time of MNB\n",
    "1. The results are due to the underlying methods used in each model. MNB uses statistics (TF/Total_Term_Number), GNB calculates the mean and variance of each TF (more computations, more time training) and LR uses Gradient Descent (more computations over many iterations).\n",
    "1. We notice that: (Fastest) MNB > LR > GNB (Slowest), with GNB being noticably worse than the rest.\n",
    "1. MNB uses the previously stored statistics to calculate the likelihood while LR calculates a dot product of the input vector and weight vector. GNB, however, is much more invloved with means, variances and more logarithmique computations.\n",
    "1. GNB is meant for features with continuous values (v in ]-inf,+inf[), so the algorithm has to deal with probability densities (implying integration), while MNB is meant for features with discrete values so the algorithm need only raw frequencies.\n",
    "1. For the problem of spam detection, the data (being text) can be seen as one feature with words as values, which is MNB's domain of application, while GNB is meant for continuous values (e.g: height, weight,...etc) making it less efficient when applied elsewhere.\n",
    "1. Storing probabilities (frequencies) allows for faster look-up and computational operations, rather than extensive statistics (like means and variances) which have to be calculated at first (training time) and at each test (test time), slowing GNB down compared to MNB.\n",
    "1. Storing probabilites is more adequate. With new data (new spam texts), you only have to increment/update old TF and add new ones if need be, while storing statistics forces you to re-calculate everything with each update. Thus updating MNB with little to no overhead compared to GNB (and LR).\n",
    "\n",
    "***NB METHODS GENERALLY DON'T OVERFIT AS WE CAN ADD A BIT OF RANDOMNESS AT THE BEGINNING BUT MOSTLY BECAUSE FEAUTRES/VARIABLES ARE SUPPOSED INDEPENDENT***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  _____    __                                              _               \n",
      " |_   _|  / _|                                            | |              \n",
      "   | |   | |_     _   _    ___    _   _      __ _    ___  | |_             \n",
      "   | |   |  _|   | | | |  / _ \\  | | | |    / _` |  / _ \\ | __|            \n",
      "  _| |_  | |     | |_| | | (_) | | |_| |   | (_| | |  __/ | |_             \n",
      " |_____| |_|      \\__, |  \\___/   \\__,_|    \\__, |  \\___|  \\__|            \n",
      "                   __/ |                     __/ |                         \n",
      "                  |___/                     |___/                          \n",
      "  _     _       _            __                                            \n",
      " | |   | |     (_)          / _|                 _                         \n",
      " | |_  | |__    _   ___    | |_    __ _   _ __  (_)                        \n",
      " | __| | '_ \\  | | / __|   |  _|  / _` | | '__|                            \n",
      " | |_  | | | | | | \\__ \\   | |   | (_| | | |     _                         \n",
      "  \\__| |_| |_| |_| |___/   |_|    \\__,_| |_|    ( )                        \n",
      "                                                |/                         \n",
      "                                                                           \n",
      "                                                                           \n",
      "                                                                           \n",
      "  _   _    ___    _   _      __ _   _ __    ___                            \n",
      " | | | |  / _ \\  | | | |    / _` | | '__|  / _ \\                           \n",
      " | |_| | | (_) | | |_| |   | (_| | | |    |  __/                           \n",
      "  \\__, |  \\___/   \\__,_|    \\__,_| |_|     \\___|                           \n",
      "   __/ |                                                                   \n",
      "  |___/                                                                    \n",
      "                    _                                                __    \n",
      "                   | |                                            _  \\ \\   \n",
      "  _ __     ___     | |__    _   _   _ __ ___     __ _   _ __     (_)  | |  \n",
      " | '_ \\   / _ \\    | '_ \\  | | | | | '_ ` _ \\   / _` | | '_ \\         | |  \n",
      " | | | | | (_) |   | | | | | |_| | | | | | | | | (_| | | | | |    _   | |  \n",
      " |_| |_|  \\___/    |_| |_|  \\__,_| |_| |_| |_|  \\__,_| |_| |_|   (_)  | |  \n",
      "                                                                     /_/   \n",
      "                                                                           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:22: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:26: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:28: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:30: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:22: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:26: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:28: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:30: SyntaxWarning: invalid escape sequence '\\_'\n",
      "C:\\Users\\inruinnomore\\AppData\\Local\\Temp\\ipykernel_1356\\4210918688.py:4: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  print(\"   | |   |  _|   | | | |  / _ \\  | | | |    / _` |  / _ \\ | __|            \")\n",
      "C:\\Users\\inruinnomore\\AppData\\Local\\Temp\\ipykernel_1356\\4210918688.py:6: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  print(\" |_____| |_|      \\__, |  \\___/   \\__,_|    \\__, |  \\___|  \\__|            \")\n",
      "C:\\Users\\inruinnomore\\AppData\\Local\\Temp\\ipykernel_1356\\4210918688.py:12: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  print(\" | __| | '_ \\  | | / __|   |  _|  / _` | | '__|                            \")\n",
      "C:\\Users\\inruinnomore\\AppData\\Local\\Temp\\ipykernel_1356\\4210918688.py:13: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  print(\" | |_  | | | | | | \\__ \\   | |   | (_| | | |     _                         \")\n",
      "C:\\Users\\inruinnomore\\AppData\\Local\\Temp\\ipykernel_1356\\4210918688.py:14: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  print(\"  \\__| |_| |_| |_| |___/   |_|    \\__,_| |_|    ( )                        \")\n",
      "C:\\Users\\inruinnomore\\AppData\\Local\\Temp\\ipykernel_1356\\4210918688.py:20: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  print(\" | | | |  / _ \\  | | | |    / _` | | '__|  / _ \\                           \")\n",
      "C:\\Users\\inruinnomore\\AppData\\Local\\Temp\\ipykernel_1356\\4210918688.py:22: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  print(\"  \\__, |  \\___/   \\__,_|    \\__,_| |_|     \\___|                           \")\n",
      "C:\\Users\\inruinnomore\\AppData\\Local\\Temp\\ipykernel_1356\\4210918688.py:26: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  print(\"                   | |                                            _  \\ \\   \")\n",
      "C:\\Users\\inruinnomore\\AppData\\Local\\Temp\\ipykernel_1356\\4210918688.py:28: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  print(\" | '_ \\   / _ \\    | '_ \\  | | | | | '_ ` _ \\   / _` | | '_ \\         | |  \")\n",
      "C:\\Users\\inruinnomore\\AppData\\Local\\Temp\\ipykernel_1356\\4210918688.py:30: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  print(\" |_| |_|  \\___/    |_| |_|  \\__,_| |_| |_| |_|  \\__,_| |_| |_|   (_)  | |  \")\n"
     ]
    }
   ],
   "source": [
    "print(\"  _____    __                                              _               \")\n",
    "print(\" |_   _|  / _|                                            | |              \")\n",
    "print(\"   | |   | |_     _   _    ___    _   _      __ _    ___  | |_             \")\n",
    "print(\"   | |   |  _|   | | | |  / _ \\  | | | |    / _` |  / _ \\ | __|            \")\n",
    "print(\"  _| |_  | |     | |_| | | (_) | | |_| |   | (_| | |  __/ | |_             \")\n",
    "print(\" |_____| |_|      \\__, |  \\___/   \\__,_|    \\__, |  \\___|  \\__|            \")\n",
    "print(\"                   __/ |                     __/ |                         \")\n",
    "print(\"                  |___/                     |___/                          \")\n",
    "print(\"  _     _       _            __                                            \")\n",
    "print(\" | |   | |     (_)          / _|                 _                         \")\n",
    "print(\" | |_  | |__    _   ___    | |_    __ _   _ __  (_)                        \")\n",
    "print(\" | __| | '_ \\  | | / __|   |  _|  / _` | | '__|                            \")\n",
    "print(\" | |_  | | | | | | \\__ \\   | |   | (_| | | |     _                         \")\n",
    "print(\"  \\__| |_| |_| |_| |___/   |_|    \\__,_| |_|    ( )                        \")\n",
    "print(\"                                                |/                         \")\n",
    "print(\"                                                                           \")\n",
    "print(\"                                                                           \")\n",
    "print(\"                                                                           \")\n",
    "print(\"  _   _    ___    _   _      __ _   _ __    ___                            \")\n",
    "print(\" | | | |  / _ \\  | | | |    / _` | | '__|  / _ \\                           \")\n",
    "print(\" | |_| | | (_) | | |_| |   | (_| | | |    |  __/                           \")\n",
    "print(\"  \\__, |  \\___/   \\__,_|    \\__,_| |_|     \\___|                           \")\n",
    "print(\"   __/ |                                                                   \")\n",
    "print(\"  |___/                                                                    \")\n",
    "print(\"                    _                                                __    \")\n",
    "print(\"                   | |                                            _  \\ \\   \")\n",
    "print(\"  _ __     ___     | |__    _   _   _ __ ___     __ _   _ __     (_)  | |  \")\n",
    "print(\" | '_ \\   / _ \\    | '_ \\  | | | | | '_ ` _ \\   / _` | | '_ \\         | |  \")\n",
    "print(\" | | | | | (_) |   | | | | | |_| | | | | | | | | (_| | | | | |    _   | |  \")\n",
    "print(\" |_| |_|  \\___/    |_| |_|  \\__,_| |_| |_| |_|  \\__,_| |_| |_|   (_)  | |  \")\n",
    "print(\"                                                                     /_/   \")\n",
    "print(\"                                                                           \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
